{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE-Experiment-4:\n",
    "\n",
    "\n",
    "#### Adversarial Autoencoder (Unsupervised training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Variational Autoencoders (VAE) the KL-divergence based regularization term applied on the latent variables  $z$  can be difficult to evaluate analytically for choice of prior distribution  $p(z)$  that are more complicated than simple distributions like Gaussian. An Adversarial Autoencoder (AAE) circumvents this problem by replacing the KL-divergence term with an adversarial loss which encourages the approximate posterior distribution  $q(z|x)$  to be closer to the prior distribution  $p(z)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment \n",
    "\n",
    "As we did with VAE, let's learn an Adversarial Autoencoder that generates digits similar to the ones in the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Import necessary modules\n",
    "##########################\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# Set parameters\n",
    "################\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "seed = 10\n",
    "\n",
    "n_classes = 10\n",
    "z_dim = 2\n",
    "X_dim = 784\n",
    "train_batch_size = 100\n",
    "valid_batch_size = train_batch_size\n",
    "N = 1000\n",
    "epochs = 5\n",
    "\n",
    "params = {}\n",
    "params['cuda'] = cuda\n",
    "params['n_classes'] = n_classes\n",
    "params['z_dim'] = z_dim\n",
    "params['X_dim'] = X_dim\n",
    "params['train_batch_size'] = train_batch_size\n",
    "params['valid_batch_size'] = valid_batch_size\n",
    "params['N'] = N\n",
    "params['epochs'] = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# Load data and create Data loaders\n",
    "###################################\n",
    "\n",
    "def load_data(data_path='./data/VAE/processed_data/processed_data/'):\n",
    "    print('loading data!')\n",
    "    trainset_labeled = pickle.load(open(data_path + \"train_labeled.p\", \"rb\"))\n",
    "    trainset_unlabeled = pickle.load(open(data_path + \"train_unlabeled.p\", \"rb\"))\n",
    "    # Set -1 as labels for unlabeled data\n",
    "    trainset_unlabeled.train_labels = torch.from_numpy(np.array([-1] * 47000))\n",
    "    validset = pickle.load(open(data_path + \"validation.p\", \"rb\"))\n",
    "\n",
    "    train_labeled_loader = torch.utils.data.DataLoader(trainset_labeled,\n",
    "                                                       batch_size=train_batch_size,\n",
    "                                                       shuffle=True, **kwargs)\n",
    "\n",
    "    train_unlabeled_loader = torch.utils.data.DataLoader(trainset_unlabeled,\n",
    "                                                         batch_size=train_batch_size,\n",
    "                                                         shuffle=True, **kwargs)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(validset, batch_size=valid_batch_size, shuffle=True)\n",
    "\n",
    "    return train_labeled_loader, train_unlabeled_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Define Networks\n",
    "#################\n",
    "\n",
    "# Encoder\n",
    "class Q_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_net, self).__init__()\n",
    "        self.lin1 = nn.Linear(X_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        # Gaussian code (z)\n",
    "        self.lin3gauss = nn.Linear(N, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        xgauss = self.lin3gauss(x)\n",
    "\n",
    "        return xgauss\n",
    "\n",
    "\n",
    "# Decoder\n",
    "class P_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(P_net, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        self.lin3 = nn.Linear(N, X_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return F.sigmoid(x)\n",
    "\n",
    "\n",
    "class D_net_gauss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D_net_gauss, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        self.lin3 = nn.Linear(N, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return F.sigmoid(self.lin3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# Utility functions\n",
    "###################\n",
    "\n",
    "def save_model(model, filename):\n",
    "    print('Best model so far, saving it...')\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def report_loss(epoch, D_loss_gauss, G_loss, recon_loss):\n",
    "    '''\n",
    "    Print loss\n",
    "    '''\n",
    "    print('Epoch-{}; D_loss_gauss: {:.4}; G_loss: {:.4}; recon_loss: {:.4}'.format(epoch,\n",
    "                                                                                   D_loss_gauss.data[0],\n",
    "                                                                                   G_loss.data[0],\n",
    "                                                                                   recon_loss.data[0]))\n",
    "\n",
    "\n",
    "def create_latent(Q, loader):\n",
    "    '''\n",
    "    Creates the latent representation for the samples in loader\n",
    "    return:\n",
    "        z_values: numpy array with the latent representations\n",
    "        labels: the labels corresponding to the latent representations\n",
    "    '''\n",
    "    Q.eval()\n",
    "    labels = []\n",
    "\n",
    "    for batch_idx, (X, target) in enumerate(loader):\n",
    "\n",
    "        X = X * 0.3081 + 0.1307\n",
    "        # X.resize_(loader.batch_size, X_dim)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        labels.extend(target.data.tolist())\n",
    "        if cuda:\n",
    "            X, target = X.cuda(), target.cuda()\n",
    "        # Reconstruction phase\n",
    "        z_sample = Q(X)\n",
    "        if batch_idx > 0:\n",
    "            z_values = np.concatenate((z_values, np.array(z_sample.data.tolist())))\n",
    "        else:\n",
    "            z_values = np.array(z_sample.data.tolist())\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return z_values, labels\n",
    "\n",
    "\n",
    "def get_X_batch(data_loader, params, size=None):\n",
    "    if size is None:\n",
    "        size = data_loader.batch_size\n",
    "\n",
    "    data_loader.batch_size = size\n",
    "\n",
    "    for X, target in data_loader:\n",
    "        break\n",
    "\n",
    "    train_batch_size = params['train_batch_size']\n",
    "    X_dim = params['X_dim']\n",
    "    cuda = params['cuda']\n",
    "\n",
    "    X = X * 0.3081 + 0.1307\n",
    "\n",
    "    X = X[:size]\n",
    "    target = target[:size]\n",
    "\n",
    "    X.resize_(size, X_dim)\n",
    "    X, target = Variable(X), Variable(target)\n",
    "\n",
    "    if cuda:\n",
    "        X, target = X.cuda(), target.cuda()\n",
    "\n",
    "    return X, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Training procedure\n",
    "####################\n",
    "\n",
    "def train(P, Q, D_gauss, P_decoder, Q_encoder, Q_generator, D_gauss_solver, data_loader):\n",
    "    '''\n",
    "    Train procedure for one epoch.\n",
    "    '''\n",
    "    TINY = 1e-15\n",
    "    # Set the networks in train mode (apply dropout when needed)\n",
    "    Q.train()\n",
    "    P.train()\n",
    "    D_gauss.train()\n",
    "\n",
    "    # Loop through the labeled and unlabeled dataset getting one batch of samples from each\n",
    "    # The batch size has to be a divisor of the size of the dataset or it will return\n",
    "    # invalid samples\n",
    "    for X, target in data_loader:\n",
    "\n",
    "        # Load batch and normalize samples to be between 0 and 1\n",
    "        X = X * 0.3081 + 0.1307\n",
    "        X.resize_(train_batch_size, X_dim)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        if cuda:\n",
    "            X, target = X.cuda(), target.cuda()\n",
    "\n",
    "        # Init gradients\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "        D_gauss.zero_grad()\n",
    "\n",
    "\n",
    "        # Reconstruction phase\n",
    "\n",
    "        z_sample = Q(X)\n",
    "        X_sample = P(z_sample)\n",
    "        recon_loss = F.binary_cross_entropy(X_sample + TINY, X.resize(train_batch_size, X_dim) + TINY)\n",
    "\n",
    "        recon_loss.backward()\n",
    "        P_decoder.step()\n",
    "        Q_encoder.step()\n",
    "\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "        D_gauss.zero_grad()\n",
    "\n",
    "\n",
    "        # Regularization phase\n",
    "\n",
    "        # Discriminator\n",
    "        Q.eval()\n",
    "        z_real_gauss = Variable(torch.randn(train_batch_size, z_dim) * 5.)\n",
    "        if cuda:\n",
    "            z_real_gauss = z_real_gauss.cuda()\n",
    "\n",
    "        z_fake_gauss = Q(X)\n",
    "\n",
    "        D_real_gauss = D_gauss(z_real_gauss)\n",
    "        D_fake_gauss = D_gauss(z_fake_gauss)\n",
    "\n",
    "        D_loss = -torch.mean(torch.log(D_real_gauss + TINY) + torch.log(1 - D_fake_gauss + TINY))\n",
    "\n",
    "        D_loss.backward()\n",
    "        D_gauss_solver.step()\n",
    "\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "        D_gauss.zero_grad()\n",
    "\n",
    "        # Generator\n",
    "        Q.train()\n",
    "        z_fake_gauss = Q(X)\n",
    "\n",
    "        D_fake_gauss = D_gauss(z_fake_gauss)\n",
    "        G_loss = -torch.mean(torch.log(D_fake_gauss + TINY))\n",
    "\n",
    "        G_loss.backward()\n",
    "        Q_generator.step()\n",
    "\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "        D_gauss.zero_grad()\n",
    "\n",
    "    return D_loss, G_loss, recon_loss\n",
    "\n",
    "\n",
    "def generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader):\n",
    "    torch.manual_seed(10)\n",
    "\n",
    "    if cuda:\n",
    "        Q = Q_net().cuda()\n",
    "        P = P_net().cuda()\n",
    "        D_gauss = D_net_gauss().cuda()\n",
    "    else:\n",
    "        Q = Q_net()\n",
    "        P = P_net()\n",
    "        D_gauss = D_net_gauss()\n",
    "\n",
    "    # Set learning rates\n",
    "    gen_lr = 0.0001\n",
    "    reg_lr = 0.00005\n",
    "\n",
    "    # Set optimizators\n",
    "    P_decoder = optim.Adam(P.parameters(), lr=gen_lr)\n",
    "    Q_encoder = optim.Adam(Q.parameters(), lr=gen_lr)\n",
    "\n",
    "    Q_generator = optim.Adam(Q.parameters(), lr=reg_lr)\n",
    "    D_gauss_solver = optim.Adam(D_gauss.parameters(), lr=reg_lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        D_loss_gauss, G_loss, recon_loss = train(P, Q, D_gauss, P_decoder, Q_encoder,\n",
    "                                                 Q_generator,\n",
    "                                                 D_gauss_solver,\n",
    "                                                 train_unlabeled_loader)\n",
    "        if epoch % 1 == 0:\n",
    "            report_loss(epoch, D_loss_gauss, G_loss, recon_loss)\n",
    "\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data!\n",
      "3000\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Train a generative model\n",
    "##########################\n",
    "\n",
    "train_labeled_loader, train_unlabeled_loader, valid_loader = load_data()\n",
    "Q, P = generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Save trained model\n",
    "####################\n",
    "\n",
    "# Save trained model\n",
    "# torch.save(Q,'./data/VAE/TrainedModels/AAE_mytraining_Q.pt')\n",
    "# torch.save(P,'./data/VAE/TrainedModels/AAE_mytraining_P.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Load trained model\n",
    "####################\n",
    "\n",
    "# Load model trained for 500 epochs\n",
    "Q_pt = torch.load('./data/VAE/TrainedModels/AAE_preTrained_Q.pt')\n",
    "P_pt = torch.load('./data/VAE/TrainedModels/AAE_preTrained_P.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Visualize reconstruction\n",
    "##########################\n",
    "\n",
    "def create_reconstruction(Q, P, data_loader, params):\n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "    X, label = get_X_batch(data_loader, params, size=1)\n",
    "\n",
    "    z = Q(X)\n",
    "    x = P(z)\n",
    "\n",
    "    img_orig = np.array(X[0].data.tolist()).reshape(28, 28)\n",
    "    img_rec = np.array(x[0].data.tolist()).reshape(28, 28)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_orig)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img_rec)\n",
    "\n",
    "\n",
    "data_loader = valid_loader    # Training data:  train_unlabeled_loader  |  Validation data:  valid_loader\n",
    "\n",
    "create_reconstruction(Q_pt, P_pt, data_loader, params)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Visualize generation\n",
    "######################\n",
    "\n",
    "def grid_plot2d(Q, P, data_loader, params):\n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "\n",
    "    cuda = params['cuda']\n",
    "\n",
    "    z1 = Variable(torch.from_numpy(np.arange(-10, 10, 1.5).astype('float32')))\n",
    "    z2 = Variable(torch.from_numpy(np.arange(-10, 10, 1.5).astype('float32')))\n",
    "    if cuda:\n",
    "        z1, z2 = z1.cuda(), z2.cuda()\n",
    "\n",
    "    nx, ny = len(z1), len(z2)\n",
    "    plt.subplot()\n",
    "    gs = gridspec.GridSpec(nx, ny, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i, g in enumerate(gs):\n",
    "        z = torch.cat((z1[i / ny], z2[i % nx])).resize(1, 2)\n",
    "        x = P(z)\n",
    "\n",
    "        ax = plt.subplot(g)\n",
    "        img = np.array(x.data.tolist()).reshape(28, 28)\n",
    "        ax.imshow(img, )\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_aspect('auto')\n",
    "\n",
    "\n",
    "grid_plot2d(Q_pt, P_pt, data_loader, params)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Plot the reconstruction loss. How does it compare against the loss that you obtained in the previous notebook which uses vanilla VAE?\n",
    "2. Try adding two more layers to the encoder and decoder and validate its generative capability. What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
