{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE-Experiment-3:\n",
    "\n",
    "\n",
    "#### Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Autoencoders (VAE) are generative models which use deep feed-forward neural networks to model complicated probability density functions $p(x)$. As discussed in the first notebook, a VAE models a complicated probability distribution as a complex deterministic transformation of a simple probability distribution. A $VAE$ model can be trained to fit a data distribution by maximizing the log-likelihood of the samples from the distribution. Specifically, as discussed in the second notebook, VAE actually maximizes a variational lower-bound to the log-likelihood. It simultaneously learns an encoder network in order to estimate the variational lower-bound during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's learn a simple fully-connected Variational Autoencoder that generates digits similar to the ones in the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Import necessary modules\n",
    "##########################\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# Set parameters\n",
    "################\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "seed = 1\n",
    "\n",
    "n_classes = 10\n",
    "z_dim = 2\n",
    "X_dim = 784\n",
    "train_batch_size = 100\n",
    "valid_batch_size = train_batch_size\n",
    "N = 1000\n",
    "epochs = 5\n",
    "\n",
    "params = {}\n",
    "params['cuda'] = cuda\n",
    "params['n_classes'] = n_classes\n",
    "params['z_dim'] = z_dim\n",
    "params['X_dim'] = X_dim\n",
    "params['train_batch_size'] = train_batch_size\n",
    "params['valid_batch_size'] = valid_batch_size\n",
    "params['N'] = N\n",
    "params['epochs'] = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# Load data and create Data loaders\n",
    "###################################\n",
    "\n",
    "def load_data(data_path='./data/VAE/processed_data/processed_data/'):\n",
    "    print('loading data!')\n",
    "    trainset_labeled = pickle.load(open(data_path + \"train_labeled.p\", \"rb\"))\n",
    "    trainset_unlabeled = pickle.load(open(data_path + \"train_unlabeled.p\", \"rb\"))\n",
    "    # Set -1 as labels for unlabeled data\n",
    "    trainset_unlabeled.train_labels = torch.from_numpy(np.array([-1] * 47000))\n",
    "    validset = pickle.load(open(data_path + \"validation.p\", \"rb\"))\n",
    "\n",
    "    train_labeled_loader = torch.utils.data.DataLoader(trainset_labeled,\n",
    "                                                       batch_size=train_batch_size,\n",
    "                                                       shuffle=True, **kwargs)\n",
    "\n",
    "    train_unlabeled_loader = torch.utils.data.DataLoader(trainset_unlabeled,\n",
    "                                                         batch_size=train_batch_size,\n",
    "                                                         shuffle=True, **kwargs)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(validset, batch_size=valid_batch_size, shuffle=True)\n",
    "\n",
    "    return train_labeled_loader, train_unlabeled_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Define Networks\n",
    "#################\n",
    "\n",
    "# Encoder\n",
    "class Q_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_net, self).__init__()\n",
    "        self.lin1 = nn.Linear(X_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        # Gaussian code (z)\n",
    "        self.lin3gauss_mean = nn.Linear(N, z_dim)\n",
    "        self.lin3gauss_logvar = nn.Linear(N, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        xgauss_mean = self.lin3gauss_mean(x)\n",
    "        xgauss_logvar = self.lin3gauss_logvar(x)\n",
    "\n",
    "        return xgauss_mean, xgauss_logvar\n",
    "\n",
    "\n",
    "# Decoder\n",
    "class P_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(P_net, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        self.lin3 = nn.Linear(N, X_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# Utility functions\n",
    "###################\n",
    "\n",
    "\n",
    "def save_model(model, filename):\n",
    "    print('Best model so far, saving it...')\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def report_loss(epoch, recon_loss):\n",
    "    '''\n",
    "    Print loss\n",
    "    '''\n",
    "    print('Epoch-{}; recon_loss: {:.4}'.format(epoch,recon_loss.data[0]))\n",
    "\n",
    "\n",
    "def create_latent(Q, loader):\n",
    "    '''\n",
    "    Creates the latent representation for the samples in loader\n",
    "    return:\n",
    "        z_values: numpy array with the latent representations\n",
    "        labels: the labels corresponding to the latent representations\n",
    "    '''\n",
    "    Q.eval()\n",
    "    labels = []\n",
    "\n",
    "    for batch_idx, (X, target) in enumerate(loader):\n",
    "\n",
    "        X = X * 0.3081 + 0.1307\n",
    "        # X.resize_(loader.batch_size, X_dim)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        labels.extend(target.data.tolist())\n",
    "        if cuda:\n",
    "            X, target = X.cuda(), target.cuda()\n",
    "        # Reconstruction phase\n",
    "        z_sample = Q(X)\n",
    "        if batch_idx > 0:\n",
    "            z_values = np.concatenate((z_values, np.array(z_sample.data.tolist())))\n",
    "        else:\n",
    "            z_values = np.array(z_sample.data.tolist())\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return z_values, labels\n",
    "\n",
    "\n",
    "def get_X_batch(data_loader, params, size=None):\n",
    "    if size is None:\n",
    "        size = data_loader.batch_size\n",
    "\n",
    "    data_loader.batch_size = size\n",
    "\n",
    "    for X, target in data_loader:\n",
    "        break\n",
    "\n",
    "    train_batch_size = params['train_batch_size']\n",
    "    X_dim = params['X_dim']\n",
    "    cuda = params['cuda']\n",
    "\n",
    "    X = X * 0.3081 + 0.1307\n",
    "\n",
    "    X = X[:size]\n",
    "    target = target[:size]\n",
    "\n",
    "    X.resize_(size, X_dim)\n",
    "    X, target = Variable(X), Variable(target)\n",
    "\n",
    "    if cuda:\n",
    "        X, target = X.cuda(), target.cuda()\n",
    "\n",
    "    return X, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE objective function and the reparameterization trick that allows us to back-propagate gradient through the sampling process in the latent layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Training procedure\n",
    "####################\n",
    "\n",
    "def train(P, Q, P_decoder, Q_encoder, data_loader):\n",
    "    '''\n",
    "    Train procedure for one epoch.\n",
    "    '''\n",
    "    TINY = 1e-15\n",
    "    # Set the networks in train mode (apply dropout when needed)\n",
    "    Q.train()\n",
    "    P.train()\n",
    "\n",
    "    # Loop through the labeled and unlabeled dataset getting one batch of samples from each\n",
    "    # The batch size has to be a divisor of the size of the dataset or it will return\n",
    "    # invalid samples\n",
    "    for X, target in data_loader:\n",
    "\n",
    "        # Load batch and normalize samples to be between 0 and 1\n",
    "        X = X * 0.3081 + 0.1307\n",
    "        X.resize_(train_batch_size, X_dim)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        if cuda:\n",
    "            X, target = X.cuda(), target.cuda()\n",
    "\n",
    "        # Init gradients\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "\n",
    "\n",
    "        # Reconstruction phase\n",
    "\n",
    "        z_mean, z_logvar = Q(X)\n",
    "        std = z_logvar.mul(0.5).exp_()\n",
    "        if cuda:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        z_sample = eps.mul(std).add_(z_mean)\n",
    "\n",
    "        X_sample = P(z_sample)\n",
    "        criterion = nn.BCELoss()\n",
    "        criterion.size_average = False\n",
    "        recon_loss = criterion(X_sample, X.resize(train_batch_size, X_dim))\n",
    "\n",
    "        # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD_element = z_mean.pow(2).add_(z_logvar.exp()).mul_(-1).add_(1).add_(z_logvar)\n",
    "        KLD_loss = torch.sum(KLD_element).mul_(-0.5)\n",
    "\n",
    "        loss =  recon_loss + KLD_loss\n",
    "\n",
    "        loss.backward()\n",
    "        P_decoder.step()\n",
    "        Q_encoder.step()\n",
    "\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "\n",
    "    return recon_loss\n",
    "\n",
    "\n",
    "def generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader):\n",
    "    torch.manual_seed(10)\n",
    "\n",
    "    if cuda:\n",
    "        Q = Q_net().cuda()\n",
    "        P = P_net().cuda()\n",
    "    else:\n",
    "        Q = Q_net()\n",
    "        P = P_net()\n",
    "\n",
    "    # Set learning rates\n",
    "    gen_lr = 0.0005\n",
    "\n",
    "    # Set optimizators\n",
    "    P_decoder = optim.Adam(P.parameters(), lr=gen_lr)\n",
    "    Q_encoder = optim.Adam(Q.parameters(), lr=gen_lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        recon_loss = train(P, Q, P_decoder, Q_encoder,\n",
    "                                                 train_unlabeled_loader)\n",
    "        if epoch % 1 == 0:\n",
    "            report_loss(epoch, recon_loss)\n",
    "\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data!\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Train a generative model\n",
    "##########################\n",
    "\n",
    "train_labeled_loader, train_unlabeled_loader, valid_loader = load_data()\n",
    "Q, P = generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Save trained model\n",
    "####################\n",
    "\n",
    "# Save trained model\n",
    "torch.save(Q,'./data/VAE/TrainedModels/VAE_mytraining_Q.pt')\n",
    "torch.save(P,'./data/VAE/TrainedModels/VAE_mytraining_P.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Load trained model\n",
    "####################\n",
    "\n",
    "# Load model trained for 200 epochs\n",
    "Q_pt = torch.load('./data/VAE/TrainedModels/VAE_preTrained_Q.pt')\n",
    "P_pt = torch.load('./data/VAE/TrainedModels/VAE_preTrained_P.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Visualize reconstruction\n",
    "##########################\n",
    "\n",
    "def create_reconstruction(Q, P, data_loader, params):\n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "    X, label = get_X_batch(data_loader, params, size=1)\n",
    "\n",
    "\n",
    "    ## Sampling from latent distribution\n",
    "\n",
    "    z_mean, z_logvar = Q(X)\n",
    "    std = z_logvar.mul(0.5).exp_()\n",
    "    if cuda:\n",
    "       eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "    else:\n",
    "       eps = torch.FloatTensor(std.size()).normal_()\n",
    "    eps = Variable(eps)\n",
    "    z_sample = eps.mul(std).add_(z_mean)\n",
    "\n",
    "\n",
    "    ## Forwarding the mean of the latent distribution\n",
    "\n",
    "    #z_mean, z_logvar = Q(X)\n",
    "    #z_sample = z_mean\n",
    "\n",
    "    x = P(z_sample)\n",
    "\n",
    "    img_orig = np.array(X[0].data.tolist()).reshape(28, 28)\n",
    "    img_rec = np.array(x[0].data.tolist()).reshape(28, 28)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_orig)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img_rec)\n",
    "\n",
    "\n",
    "data_loader = valid_loader    # Training data:  train_unlabeled_loader  |  Validation data:  valid_loader\n",
    "\n",
    "create_reconstruction(Q_pt, P_pt, data_loader, params)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Visualize generation\n",
    "######################\n",
    "\n",
    "def grid_plot2d(Q, P, params):\n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "\n",
    "    cuda = params['cuda']\n",
    "\n",
    "    z1 = Variable(torch.from_numpy(np.arange(-1, 1, 0.15).astype('float32')))\n",
    "    z2 = Variable(torch.from_numpy(np.arange(-1, 1, 0.15).astype('float32')))\n",
    "\n",
    "    if cuda:\n",
    "        z1, z2 = z1.cuda(), z2.cuda()\n",
    "\n",
    "    nx, ny = len(z1), len(z2)\n",
    "    plt.subplot()\n",
    "    gs = gridspec.GridSpec(nx, ny, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i, g in enumerate(gs):\n",
    "        z = torch.cat((z1[i / ny], z2[i % nx])).resize(1, 2)\n",
    "        x = P(z)\n",
    "\n",
    "        ax = plt.subplot(g)\n",
    "        img = np.array(x.data.tolist()).reshape(28, 28)\n",
    "        ax.imshow(img, )\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_aspect('auto')\n",
    "\n",
    "\n",
    "grid_plot2d(Q_pt, P_pt, params)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-shaped distribution\n",
    "\n",
    "Now, remember the C-shaped distribution from the first notebook? Here it is again, Can you learn a Variational Autoencoder that models this distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Reset workspace for next experiment\n",
    "#####################################\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Import necessary modules\n",
    "##########################\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# Set parameters\n",
    "################\n",
    "\n",
    "cuda = True\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "seed = 1\n",
    "\n",
    "z_dim = 2\n",
    "X_dim = 2\n",
    "train_batch_size = 100\n",
    "valid_batch_size = train_batch_size\n",
    "N = 100\n",
    "epochs = 100\n",
    "\n",
    "params = {}\n",
    "params['cuda'] = cuda\n",
    "params['z_dim'] = z_dim\n",
    "params['X_dim'] = X_dim\n",
    "params['train_batch_size'] = train_batch_size\n",
    "params['valid_batch_size'] = valid_batch_size\n",
    "params['N'] = N\n",
    "params['epochs'] = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Define Data-loader\n",
    "####################\n",
    "\n",
    "def data_loader_Cdist(numSamples):\n",
    "\n",
    "    z = np.random.randn(numSamples,2).astype(np.float32)        # Sample from Gaussian distribution\n",
    "    z1 = z[:,0]\n",
    "    z2 = z[:,1]\n",
    "    os = 10\n",
    "    ost = np.pi/2\n",
    "    x1 = -(1.5*os+z1)*(np.sin(z2+ost))\n",
    "    x2 = (os+z1)*np.cos(z2+ost)\n",
    "\n",
    "    x = np.concatenate([np.expand_dims(x1,1),np.expand_dims(x2,1)],1)\n",
    "    dummy = np.zeros(len(x))\n",
    "\n",
    "    return torch.from_numpy(x), torch.from_numpy(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Define Networks\n",
    "#################\n",
    "\n",
    "# Encoder\n",
    "class Q_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_net, self).__init__()\n",
    "        self.lin1 = nn.Linear(X_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        # Gaussian code (z)\n",
    "        self.lin3gauss_mean = nn.Linear(N, z_dim)\n",
    "        self.lin3gauss_logvar = nn.Linear(N, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "        xgauss_mean = self.lin3gauss_mean(x)\n",
    "        xgauss_logvar = self.lin3gauss_logvar(x)\n",
    "\n",
    "        return xgauss_mean, xgauss_logvar\n",
    "\n",
    "\n",
    "# Decoder\n",
    "class P_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(P_net, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        self.lin3 = nn.Linear(N, X_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# Utility functions\n",
    "###################\n",
    "\n",
    "\n",
    "def save_model(model, filename):\n",
    "    print('Best model so far, saving it...')\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def report_loss(epoch, recon_loss):\n",
    "    '''\n",
    "    Print loss\n",
    "    '''\n",
    "    print('Epoch-{}; recon_loss: {:.4}'.format(epoch,recon_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Training procedure\n",
    "####################\n",
    "\n",
    "def train(P, Q, P_decoder, Q_encoder, data_loader):\n",
    "    '''\n",
    "    Train procedure for one epoch.\n",
    "    '''\n",
    "    TINY = 1e-15\n",
    "    # Set the networks in train mode (apply dropout when needed)\n",
    "    Q.train()\n",
    "    P.train()\n",
    "\n",
    "    # Loop through the labeled and unlabeled dataset getting one batch of samples from each\n",
    "    # The batch size has to be a divisor of the size of the dataset or it will return\n",
    "    # invalid samples\n",
    "    for it in range(100):\n",
    "\n",
    "        X, target = data_loader(train_batch_size)\n",
    "\n",
    "        # Load batch and normalize samples to be 0-mean and unit-std\n",
    "        X = (X + 4.55)/ 8.08\n",
    "        X.resize_(train_batch_size, X_dim)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        if cuda:\n",
    "            X, target = X.cuda(), target.cuda()\n",
    "\n",
    "        # Init gradients\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "\n",
    "\n",
    "        # Reconstruction phase\n",
    "\n",
    "        z_mean, z_logvar = Q(X)\n",
    "        std = z_logvar.mul(0.5).exp_()\n",
    "        if cuda:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        z_sample = eps.mul(std).add_(z_mean)\n",
    "\n",
    "        X_sample = P(z_sample)\n",
    "        criterion = nn.MSELoss()\n",
    "        criterion.size_average = False\n",
    "        recon_loss = criterion(X_sample, X.resize(train_batch_size, X_dim))\n",
    "\n",
    "        # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD_element = z_mean.pow(2).add_(z_logvar.exp()).mul_(-1).add_(1).add_(z_logvar)\n",
    "        KLD_loss = torch.sum(KLD_element).mul_(-0.5)\n",
    "\n",
    "        loss =  recon_loss + KLD_loss\n",
    "\n",
    "        loss.backward()\n",
    "        P_decoder.step()\n",
    "        Q_encoder.step()\n",
    "\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "\n",
    "    return recon_loss\n",
    "\n",
    "\n",
    "def generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader):\n",
    "    torch.manual_seed(10)\n",
    "\n",
    "    if cuda:\n",
    "        Q = Q_net().cuda()\n",
    "        P = P_net().cuda()\n",
    "    else:\n",
    "        Q = Q_net()\n",
    "        P = P_net()\n",
    "\n",
    "    # Set learning rates\n",
    "    gen_lr = 0.0001\n",
    "\n",
    "    # Set optimizators\n",
    "    P_decoder = optim.Adam(P.parameters(), lr=gen_lr)\n",
    "    Q_encoder = optim.Adam(Q.parameters(), lr=gen_lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #x,dummy = train_unlabeled_loader(train_batch_size)\n",
    "        recon_loss = train(P, Q, P_decoder, Q_encoder,\n",
    "                                                 train_unlabeled_loader)\n",
    "        if epoch % 10 == 0:\n",
    "            report_loss(epoch, recon_loss)\n",
    "\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0; recon_loss: 142.0\n",
      "Epoch-10; recon_loss: 104.6\n",
      "Epoch-20; recon_loss: 91.57\n",
      "Epoch-30; recon_loss: 63.06\n",
      "Epoch-40; recon_loss: 79.11\n",
      "Epoch-50; recon_loss: 72.9\n",
      "Epoch-60; recon_loss: 76.56\n",
      "Epoch-70; recon_loss: 75.58\n",
      "Epoch-80; recon_loss: 70.77\n",
      "Epoch-90; recon_loss: 99.99\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Train a generative model\n",
    "##########################\n",
    "\n",
    "Q, P = generate_model(data_loader_Cdist, data_loader_Cdist, data_loader_Cdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIDoEEnyCIIgOgQSfIAiiQ/j/AWGjFCZk5xoPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feeb450f4d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################\n",
    "# Visualize generated distribution\n",
    "##################################\n",
    "\n",
    "def generate_Cdist(numSamples):\n",
    "\n",
    "    z = np.random.randn(numSamples,2).astype(np.float32)        # Sample from Gaussian distribution\n",
    "\n",
    "    z = Variable(torch.from_numpy(z))\n",
    "    if cuda:\n",
    "        z = z.cuda()\n",
    "\n",
    "    x = P(z)\n",
    "\n",
    "    return x\n",
    "\n",
    "x = generate_Cdist(5000)\n",
    "x = np.array(x.data.tolist())\n",
    "x = x*8.08 - 4.55\n",
    "x1 = x[:,0]\n",
    "x2 = x[:,1]\n",
    "\n",
    "plt.scatter(x1,x2)\n",
    "plt.hold()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE seems to have done a near-decent job at modelling the C-shaped distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. In function 'grid_plot2d()', we visualize the ability of the decoder to generate MNIST digits when as we change samples from the latent distribution. Play with it by varying its range and visualizing the outputs. You can use the pretrained decoder model for this. What do you observe?\n",
    "\n",
    "2. In the MNIST example, plot the reconstruction loss and the KL Divergence, as training progresses. How many epochs are required to get reasonable results?\n",
    "\n",
    "3. In the C-shaped distribution, tune the VAE so that it models the distribution even better.\n",
    "4. VAEs have a problem that the generations are not sharp enough, although meaningful. Can you come up with an idea to make them sharper? Implement it!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
